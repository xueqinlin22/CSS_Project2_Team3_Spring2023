---
title: 'Project 6: Randomization and Matching'
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eﬀects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
#install.packages("cobalt")
#install.packages("dplyr")
#install.packages("randomizr")
#install.packages("egg")
#install.packages("ggplotify")
library(randomizr)
library(dplyr)
library(cobalt)
library(tidyverse)
library(MatchIt)
library(ggplot2)
library(egg)
library(ggplotify)
set.seed(12345)

# Load ypsps data
ypsps <- read_csv('data/ypsps.csv')
head(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

```{r}
# Generate a vector that randomly assigns each unit to treatment/control

ypsps$Status <- as.numeric(rbernoulli(1254, p = 0.5))

table(ypsps$Status)
head(ypsps)
# Choose a baseline covariate (use dplyr for this)
# We will use parent_Money as the covariate.

# Visualize the distribution by treatment/control (ggplot)

ggplot(ypsps, aes(x = parent_Money, fill = factor(Status))) + 
  geom_bar(position = "stack") +
  facet_grid(Status~.) + 
  scale_fill_discrete(name="Status", labels = c("Control", "Treatment")) +
  xlab("Parent Income Level") +
  ylab("Count") +
  ggtitle("Distribution of Parent Income among Treatment and Control group")

chisq.test(table(ypsps$Status, ypsps$parent_Money))
```


```{r}
# Simulate this 10,000 times (monte carlo simulation - see R Refresher for a hint)

## Monte carlo stimulation
stimulation <- replicate(10000, {
ypsps$Status <- as.numeric(rbernoulli(1254, p = 0.5))

#calculate the proportion of parent_Money = 1 in the treatment group
prop_money1_treatment <-mean(ypsps$parent_Money[ypsps$Status==1] == 1, na.rm = TRUE)

#calculate the proportion of parent_Money = 1 in the control group
prop_money0_treatment <-mean(ypsps$parent_Money[ypsps$Status==0] == 1, na.rm = TRUE)

#calculate the difference:
prop_money1_treatment - prop_money0_treatment
 })

## visualize the distribution
 ggplot()+
  geom_density(aes(x = stimulation), fill="blue", alpha = 0.5) +
  xlab("Parent Income Level") +
  ylab("Density") +
  ggtitle("Monte Carlo Simulation of Distribution of Parent Income among = 1 in the Treatment Group")

```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

\textbf{The difference in the proportion of high parental income groups between two treatment and control arm have a normal distribution centered on zero. This means that of all the stimulation, a majority of times the covariate is balanced, yet the unbalanced situation does exist.}

 \item \textbf{Why the balance is not guaranteed under randomization?}
 \end{enumerate}

\textbf{The purpose of randomization is to help reduce researcher bias in terms of sample selection for treatment and control groups, but it does not imply that balance will be achieved.}

# Propensity Score Matching

## One Model
Select covariates that you think best represent the “true” model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Hint: Use matchit() function. It has “estimand” parameter, which can be set to “ATT” if you want to calculate the Average Treatment Effect on the Treated(ATT).

Plot the balance of the covariates (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold. Hint: You can use matchit() function here, however, bal.tab() in “cobalt” package might make this task easier.

```{r}
# Select covariates that represent the "true" model for selection, fit model
# We will use the following covariates: 1. parent_Money, 2. parent_Employ, 3. parent_EducHH, 4. parent_EducW, 5. parent_OwnHome, 6. parent_Knowledge, 7. parent_Gen, 8. parent_Race, 9. student_LifeWish, 10. student_Knowledge [additional variables to consider : 11. student_Race, 12. student_Gen]

# fit a model with 3:1 nearest neighbor matching without replacement and on the propensity score to calculate ATT
match_ps_att <- matchit(college ~ parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = ypsps, method = "nearest", ratio = 3, distance = "glm", estimand = "ATT")

#get a description of the type of matching performed
match_ps_att #get a description of the type of matching performed
summary(match_ps_att, un = TRUE, standardize = TRUE)

match_ps_att_data <- match.data(match_ps_att)
  
lm_ps_att <- lm(student_ppnscal ~ college + parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = match_ps_att_data, weights = weights)

lm_ps_att_summ <- summary(lm_ps_att)
lm_ps_att_summ

ATT_ps <- lm_ps_att_summ$coefficients["college", "Estimate"]
ATT_ps
```


```{r}
# Plot the balance for the top 10 covariates
model_att_out <- MatchIt::matchit(college ~ parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = ypsps, method = "nearest", ratio = 3,  distance = "glm", estimand = "ATT", replace = TRUE)

model_att_out #get a description of the type of matching performed
bal.tab(model_att_out, un = TRUE)
summary(model_att_out, un = TRUE, standardize = TRUE)

bal.plot(model_att_out, "parent_Money", which = "both")
bal.plot(model_att_out, "parent_Employ", which = "both")
bal.plot(model_att_out, "parent_EducHH", which = "both")
bal.plot(model_att_out, "parent_EducW", which = "both")
bal.plot(model_att_out, "parent_OwnHome", which = "both")
bal.plot(model_att_out, "parent_Knowledge", which = "both")
bal.plot(model_att_out, "parent_Race", which = "both")
bal.plot(model_att_out, "student_Knowledge", which = "both")

love.plot(model_att_out, binary = "std")
# Report the overall balance and the number of covariates that meet the balance threshold
# !! Elaine's answer: not sure where to look for an overall balance index but will check with TA in class
# 8 out of the 10 covariates we choose met the balance threshold of p-score ≤ .1.
```

## Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r message=FALSE}
# Remove post-treatment covariates

ypsps_clean1 <- ypsps %>% select(!contains("1973"))
ypsps_clean2 <- ypsps_clean1 %>% select(!contains("1982"))%>% select(!c(parent_HHCollegePlacebo, parent_GPHighSchoolPlacebo))

head (ypsps)
head (ypsps_clean2)

# Randomly select features
set.seed(12345)
n_simulations <- 1000 # We ran 1000 instead given R Studio crashed many times when we attempted to run 10000 simulations. 

res <- list()
ATT_ps_1 <- list()
prop_cov <- list()
mean_percent_1 <- tibble()
#size <- list()
count <- list()
#features <- list()

#res_lm <- list()

for (i in 1:n_simulations) {
  n_size <- sample(1:108,1)
  #size[i] <- n_size
  selected_features <- sample(12:120,n_size)
  #features[i] <- selected_features
  dt <- ypsps_clean2[,selected_features]
  match_ps_att <- matchit(as.formula(paste0("college ~ ",paste0(colnames(dt), collapse= "+" ))), data = ypsps_clean2, method = "nearest", ratio = 3, distance ="glm", estimand = "ATT")
  
  match_ps_att_data <- match.data(match_ps_att)
  lm_ps_att <- lm(as.formula(paste0("student_ppnscal ~ ", paste0("college+", paste0(colnames(dt), collapse= "+")))), data = match_ps_att_data, weights = weights)
  lm_ps_att_summ <- summary(lm_ps_att)
  #res_lm[i] <- lm_ps_att
  ATT_ps <- lm_ps_att_summ$coefficients["college", "Estimate"]
  ATT_ps_1[i]=ATT_ps # store ATT 
  
  model_att_out <- MatchIt::matchit(as.formula(paste0("college ~ ",paste0(colnames(dt), collapse= "+" ))), data = ypsps_clean2, method = "nearest", ratio = 3, distance ="glm", estimand = "ATT")
  res[[i]] = model_att_out
  sdm_table <- bal.tab(model_att_out,un= TRUE,m.threshold = 0.1)
  smd_tab <- sdm_table[["Balance"]] %>% filter(!row_number() %in% c(1))
  count[i] <- sum(smd_tab["M.Threshold"]=="Balanced, <0.1")
  prop_cov[i] <- (sum(smd_tab["M.Threshold"]=="Balanced, <0.1"))/n_size # store the proportion of the covariates that meets the threshold 
  
  mean_percent <- abs((sdm_table[["Balance"]][["Diff.Adj"]]-sdm_table[["Balance"]][["Diff.Un"]])/sdm_table[["Balance"]][["Diff.Un"]]) * 100
  mean_percent_1 <- rbind(mean_percent_1, tibble(mean_percent))    # store mean percent improvement  
                            
}

ATT_ps_1
prop_cov
mean_percent_1

sum(prop_cov >0.9)

```

```{r}
# Plot ATT v. proportion. More specifically: Plot all of the ATTs against all of the balanced covariate proportions.

ATT_ps_1 <- as.data.frame(do.call(rbind, ATT_ps_1))
ATT_ps_1 <- rename(ATT_ps_1, ATT_ps = V1)

prop_cov <- as.data.frame(do.call(rbind, prop_cov))
prop_cov <- rename(prop_cov, prop_cov = V1)

ATT_ps_1_prop_cov <- cbind(ATT_ps_1, prop_cov)
ATT_ps_1_prop_cov

class(ATT_ps_1_prop_cov$prop_cov)
class(ATT_ps_1_prop_cov$ATT_ps)

plot <- ggplot(ATT_ps_1_prop_cov, aes(x=ATT_ps ,y = prop_cov)) +
  geom_point() +
  geom_smooth() +
  xlab("ATT") +
  ylab("balanced covariate proportions") +
  ggtitle("ATT against balanced covariate proportions")

print(plot)
```
```{r}
att_plt <- ggplot(ATT_ps_1, aes(x = ATT_ps)) +
  geom_density(fill = "blue", alpha = 0.5) +
  ggtitle("ATT Distribution") +
  xlab("ATT Value") +
  ylab("Density")

print(att_plt)
```


```{r}
# 10 random covariate balance plots (hint try gridExtra). More specifically, choose 10 random models and plot their covariate balance plots.
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!

ten_random <- sample(1:length(res), 10)
ten_random <- c(18,1,8,4,14,7,9,12,5,10)
ten_plot = list()

for (i in ten_random) {
  print(i)
  ten_plot[[i]] <- love.plot(res[[i]], binary = "std")
  print(ten_plot[[i]])
  }
```

```{r}
grid.arrange(grobs = ten_plot[!sapply(ten_plot,is.null)], ncol = 2, widths = c(10, 10))
```

## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
    \item \textbf{Given that there is no set threshold for what is considered "high" and that it depends on the specific context and research question, in this project, we arbitrarily choose the threshold > .8 as high proportion of balanced covariates. Based on the simulation (n = 1,000) results above, only 165 of the 1000 simulated model met the threshold, suggesting that these 165 models have more than 80% of covariates that meet the standardized mean difference smaller than or equal to .1 threshold. We are somewhat concerned about this because only 16.5% of the simulated models meet the threshold. In our opinion, with only slightly more than 15% of the simulated model meet the threshold, we are concerned that our model may likely lead to biased estimates of treatment effects, making it difficulty to draw valid or generalizable conclusions from our analysis.}
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    \item \textbf{Based on the ATT distribution of the 1,000 simulated models, it is observed that the distribution is slightly right-skewed. This indicates that some participants in the treatment group may have had a large positive effect, while most individuals had a smaller effect, or there may be presence of extreme outliers or influential observations. Furthermore, the mean at approximately .9 suggests that the treatment is still associated with a positive effect. However, it is essential to note that the distribution range is wide, from approximately .85 to 1.6. Therefore, it is possible that treatment effects may be highly variable across different units in the sample. In summary, the observed ATT distribution raises some potential concerns regarding the variability of treatment effects among the units.}
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    \item \textbf{The covariate balance plots of the 10 randomly chosen simulated models did not yield similar numbers for the same covariates. However, this is not a cause for concern as each model has a different set of covariates and their combinations may vary widely. Therefore, the differences observed in the balance plots could be attributed to these variations.}
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:
```{r}
# Alternative Model (Full Optimal Mahalanobis Matching)

match_full_att <- matchit(college ~ parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = ypsps, method = "full", distance = "mahalanobis")
summary(match_full_att)

match_full_att_data <- match.data(match_full_att)
lm_full_att <- lm(student_ppnscal ~ college + parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = match_full_att_data, weights = weights)
lm_full_att_summ <- summary(lm_full_att)
lm_full_att_summ

ATT_full <- lm_full_att_summ$coefficients["college", "Estimate"]
ATT_full
```
```{r}
# Plot the balance for the top 10 covariates
match_att_out <- MatchIt::matchit(college ~ parent_Money + parent_Employ + parent_EducHH + parent_EducW + parent_OwnHome + parent_Knowledge + parent_Gen + parent_Race + student_LifeWish + student_Knowledge, data = ypsps, "full", distance = "mahalanobis", estimand = "ATT")

match_att_out #get a description of the type of matching performed
bal.tab(match_att_out, un = TRUE)
summary(match_att_out, un = TRUE, standardize = TRUE)

bal.plot(match_att_out, "parent_Money", which = "both")
bal.plot(match_att_out, "parent_Employ", which = "both")
bal.plot(match_att_out, "parent_EducHH", which = "both")
bal.plot(match_att_out, "parent_EducW", which = "both")
bal.plot(match_att_out, "parent_OwnHome", which = "both")
bal.plot(match_att_out, "parent_Knowledge", which = "both")
bal.plot(match_att_out, "parent_Race", which = "both")
bal.plot(match_att_out, "student_Knowledge", which = "both")

love.plot(match_att_out, binary = "std")

```

```{r}
# Remove post-treatment covariates

#ypsps_clean1 <- ypsps %>% select(!contains("1973"))
#ypsps_clean2 <- ypsps_clean1 %>% select(!contains("1982"))%>% select(!c(parent_HHCollegePlacebo, parent_GPHighSchoolPlacebo))

#head (ypsps)
#head (ypsps_clean2)

# Randomly select features
n_simulations <- 1000 # My R studio died when I was attempting to run 10000 simulations.So we decided to run 1000 simulations. 

res_1 <- list()
ATT_full_1 <- list()
prop_cov_1 <- list()
mean_percent_2 <- tibble()
#size_1 <- list()
#count_1 <- list()
#features_1 <- list()

for (i in 1:n_simulations) {
  n_size_1 <- sample(1:108,1)
  #size_1[i] <- n_size_1
  selected_features_1 <- sample(12:120,n_size_1)
  #features_1[i] <- selected_features_1
  df <- ypsps_clean2[,selected_features_1]
  full_att <- matchit(as.formula(paste0("college ~ ",paste0(colnames(df), collapse= "+" ))), data = ypsps_clean2, method = "full", distance = "mahalanobis", estimand = "ATT")
  res_1[i] = full_att
  full_att_data <- match.data(full_att)
  lm_full_att <- lm(as.formula(paste0("student_ppnscal ~ ", paste0("college+", paste0(colnames(dt), collapse= "+")))), data = full_att_data, weights = weights)
  lm_full_att_summ <- summary(lm_full_att)
  ATT_full <- lm_full_att_summ$coefficients["college", "Estimate"]
  ATT_full_1[i]=ATT_full # store ATT 
  
  full_att_out <- MatchIt::matchit(as.formula(paste0("college ~ ",paste0(colnames(df), collapse= "+" ))), data = ypsps_clean2, method = "full", distance = "mahalanobis",  estimand = "ATT")
  fullsdm_table <- bal.tab(full_att_out,un= TRUE,m.threshold = 0.1)
  prop_cov_1[i] <- (fullsdm_table[["Balanced.mean.diffs"]][1,1])/n_size_1 # store the proportion of the covariates that meets the threshold 
  
  mean_percent_full <- abs((fullsdm_table[["Balance"]][["Diff.Adj"]]-fullsdm_table[["Balance"]][["Diff.Un"]])/fullsdm_table[["Balance"]][["Diff.Un"]]) * 100
  mean_percent_2 <- rbind(mean_percent_full, tibble(mean_percent_full))    # store mean percent improvement  
                            
}

ATT_full_1 

mean_percent_2

prop_cov_1

sum(prop_cov_1 > 0.8)
```



```{r}
# Plot ATT v. proportion. More specifically: Plot all of the ATTs against all of the balanced covariate proportions.

ATT_full_2 <- as.data.frame(do.call(rbind, ATT_full_1))
ATT_full_2 <- rename(ATT_full_2, ATT_full = V1)

prop_cov_2 <- as.data.frame(do.call(rbind, prop_cov_1))
prop_cov_2 <- rename(prop_cov_2, prop_cov = V1)

ATT_full_2_prop_cov <- cbind(ATT_full_2, prop_cov_2)
view(ATT_full_2_prop_cov)

class(ATT_full_2_prop_cov$prop_cov)
class(ATT_full_2_prop_cov$ATT_full)

plot_1 <- ggplot(ATT_full_2_prop_cov, aes(x=ATT_full ,y = prop_cov)) +
  geom_point() +
  geom_smooth() +
  xlab("ATT") +
  ylab("balanced covariate proportions") +
  ggtitle("ATT against balanced covariate proportions for Full Optimal Model")

print(plot_1)
```


```{r}

att_plt <- ggplot(ATT_full_2, aes(x = ATT_full)) +
  geom_density(fill = "blue", alpha = 0.5) +
  ggtitle("ATT Distribution for Full Optimal Model") +
  xlab("ATT Value") +
  ylab("Density")

print(att_plt)


```


```{r}
# 10 random covariate balance plots (hint try gridExtra). More specifically, choose 10 random models and plot their covariate balance plots.
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
ten_random <- sample(1:length(res_1), 10)
#ten_random <- c(18,1,8,4,14,7,9,12,5,10)
ten_plot_1 = list()

for (i in ten_random) {
  print(i)
  ten_plot_1[[i]] <- love.plot(res[[i]], binary = "std")
  print(ten_plot_1[[i]])
}

```


```{r}
grid.arrange(grobs = ten_plot_1[!sapply(ten_plot_1,is.null)], ncol = 2, widths = c(10, 10))

```


```{r}
# Visualization for distributions of percent improvement


ggplot(mean_percent_1, aes(x = mean_percent))+
  geom_density(fill="blue", alpha = 0.5) +
  xlab("Mean Percent Improvement") +
  ylab("Density") +
  ggtitle("Mean Percent Improvement Distribution for Nearest neighbors model")

    
 # Visualization for distributions of percent improvement
ggplot(mean_percent_2, aes(x=mean_percent_full)) + 
   geom_density(alpha=0.5, colour="black", fill="red")+
   xlab("Mean Percent Improvement") +
   ylab("Density") +
   ggtitle("Mean Percent Improvement Distribution for Full Optimal model")
  
    

```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
    \item \textbf{Your Answer: The alternative matching method did not have more runs with higher proportions compared to the nearest neighbors model.}
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    \item \textbf{Your Answer: Based on the results of distributions of the percent improvement in balance between two models, propensity score matching appear to have a better results. Its distribution is more condensed and with larger mean percent improvement.}
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item Why might it be a good idea to do matching even if we have a randomized or as-if-random design?
    \item \textbf{Although randomized or as-if-random designs aim to distribute covariates evenly between the treatment and control groups, there may still be some imbalance. Therefore, matching is helpful to improve covariate balance and reduce the variance of the estimated treatment effect. Additionally, by matching, we can identify confounding variables that may have been missed or unobserved in the initial study design, further improving the precision and accuracy of treatment effect estimates. Even with randomized designs, chance imbalances in covariates can still occur, and matching can help researchers mitigate this issue and improve the validity of study results.}
    \item The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?
    \item \textbf{Your Answer: Yes. Researchers have proposed using other machine learning algorithms to estimate propensity scores. For example, Linden and colleagues (2017) proposed the use of classification tree analysis to generate propensity scores. The decision tree model provides more accurate and parsimonious decision rules, which are easier to display and interpret.Westreich et al.(2010) reviewed alternative ways to regression model for propensity score. For example, compared to regression model, other machine learning models, such as neural network, are able to construct a model that incorporates valuable information from each covariate when the number of covariates exceeds the number of data points. Westreich et al.(2010) "believe that boosting methods have the best potential for estimation of propensity scores because of the power and flexibility of their naive implementations. CART has similar promise, but the need for pruning in decision tree algorithms makes CART less attractive. Neural networks and SVMs also show less potential because of the expertise involved in tuning the learning algorithm."}
\end{enumerate}